{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "agnews_dataset_train = AG_NEWS(root = \"test\", split='train')\n",
    "agnews_dataset_test = AG_NEWS(root = \"test\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(agnews_dataset_train)\n",
    "test_iter = iter(agnews_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(root = \"test\", split='train')  # 后面会被冲掉，所以要重新写\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])  # 这个就是用来遍历整个数据集 然后把所有英文单词转成向量表示\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class AgNewsDataset(Dataset):\n",
    "    def __init__(self,mode=\"train\") :\n",
    "        if mode == \"test\":\n",
    "            self.labels = [label for label,text in AG_NEWS(root = \"test\", split='test')]\n",
    "            self.texts = [text for label,text in AG_NEWS(root = \"test\", split='test')]\n",
    "        else:\n",
    "            self.labels = [label for label,text in AG_NEWS(root = \"test\", split='train')]\n",
    "            self.texts = [text for label,text in AG_NEWS(root = \"test\", split='train')]\n",
    "        self.maxLen = 80\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # print(\"get!\")  # 用一个变量去接，不要改数据库里的数据\n",
    "        label = torch.tensor(int(self.labels[index]) - 1)\n",
    "        text = torch.tensor(vocab(tokenizer(self.texts[index])))\n",
    "        # 补齐和裁剪\n",
    "        if len(text) <= self.maxLen:\n",
    "            text = torch.cat((text, torch.zeros(self.maxLen - len(text), dtype=torch.int64)))\n",
    "        else:\n",
    "            text = text[:80]\n",
    "        return text, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = AgNewsDataset()\n",
    "test = AgNewsDataset(mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1355,  1236,   517, 13945,    38,  1416,    13,  2199,     1,   172,\n",
       "            14,  2199,     1,   172,    15,   832,   124,  5951,   113,     5,\n",
       "          2539,     7,  1232,     3,     8,    23,   571,    11,  2444,  1687,\n",
       "           439,    69,    85,     3,     2,   100,    26,    60,     3,  7126,\n",
       "             2,   347,    21,  2566,    29,     5, 26470,  3676,     1,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[7]\n",
    "# 不用__getitem(7)__ 这样读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "num_hiddens = 128\n",
    "emb_dim = 256\n",
    "batch_size = 4\n",
    "num_step = 4\n",
    "output_dim = 4\n",
    "epoch = 5\n",
    "num_class = len(set([label for (label, text) in AG_NEWS(root = \"test\", split='test')]))  # 前面的迭代器不能用，重新执行\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size,shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 80])\n"
     ]
    }
   ],
   "source": [
    "for text,label in train_dataloader:\n",
    "    print(text.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class AgNewsClassification(nn.Module):\n",
    "    def __init__(self, num_class = num_class) :\n",
    "        super(AgNewsClassification,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.rnn = nn.RNN(emb_dim,num_hiddens)\n",
    "        # self.lstm = nn.LSTM(emb_dim,num_hiddens)\n",
    "        # self.gru = nn.GRU(emb_dim,num_hiddens)\n",
    "        self.fc = nn.Linear(num_hiddens,num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        out = torch.mean(output, dim=1)\n",
    "        logits = self.fc(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1697,  0.7265,  0.0540,  0.6336]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AgNewsClassification()\n",
    "model(train[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class rnn_try(nn.Module):\n",
    "    def __init__(self, emb_dim = 128,num_hiddens = 256) :\n",
    "        super(rnn_try,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.xh = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.hh = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hq = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x:torch.Tensor,h:torch.Tensor):  # X:[batch,seq_length,embedding]\n",
    "        out =[]\n",
    "        length = x.shape[1]\n",
    "        x = x.permute(1,0,2)\n",
    "        for i in range(length):\n",
    "            out1 = self.xh(x[i])\n",
    "            out2 = self.hh(h)\n",
    "            ht = self.act(out1+out2)\n",
    "            ot = self.hq(ht)\n",
    "            print(\"ot:\",ot.shape)\n",
    "            out.append(ot)\n",
    "            h = ht\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    " # 接分类头版本\n",
    "class rnn_try(nn.Module):\n",
    "    def __init__(self, emb_dim = 128,num_hiddens = 256) :\n",
    "        super(rnn_try,self).__init__()\n",
    "        # 封装的RNN里面自带有embedding，自己写要加embedding层\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.xh = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.hh = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hq = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.act = nn.Tanh()\n",
    "        self.classifier = nn.Linear(num_hiddens,num_class)\n",
    "\n",
    "    def forward(self, x:torch.Tensor,h:torch.Tensor):  # X:[batch,seq_length,embedding]\n",
    "        out =[]\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        length = x.shape[1]\n",
    "        x = x.permute(1,0,2)\n",
    "        for i in range(length):\n",
    "            out1 = self.xh(x[i])\n",
    "            out2 = self.hh(h)\n",
    "            ht = self.act(out1+out2)\n",
    "            ot = self.hq(ht)\n",
    "            # print(\"ot:\",ot.shape)\n",
    "            out.append(ot)\n",
    "            h = ht\n",
    "        return self.classifier(out[-1])\n",
    "    # 只输出最后时间步Ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class lstm_try(nn.Module):\n",
    "    def __init__(self, emb_dim = 128,num_hiddens = 256) :\n",
    "        super(lstm_try,self).__init__()\n",
    "        self.xf = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xi = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xo = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xc = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.hi = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hf = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.ho = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hc = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hq = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.act1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x:torch.Tensor,h:torch.Tensor,c:torch.Tensor):  # X:[batch,seq_length,embedding]\n",
    "        out =[]\n",
    "        length = x.shape[1]\n",
    "        x = x.permute(1,0,2)\n",
    "        for i in range(length):\n",
    "            Ft = self.act1(self.xf(x[i])+self.hf(h))\n",
    "            It = self.act1(self.xi(x[i])+self.hi(h))\n",
    "            Ot = self.act1(self.xo(x[i])+self.ho(h))\n",
    "            Ct1 = self.act2(self.xc(x[i])+self.hc(h))\n",
    "            Ct = Ft * c + Ct1 * It\n",
    "            ht = Ot * self.act2(Ct)\n",
    "            Y = self.hq(ht)\n",
    "            out.append(Y)\n",
    "            h = ht\n",
    "            c = Ct\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class lstm_try(nn.Module):\n",
    "    def __init__(self, emb_dim = 128,num_hiddens = 256) :\n",
    "        super(lstm_try,self).__init__()\n",
    "        self.embedded = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.xf = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xi = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xo = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xc = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.hi = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hf = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.ho = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hc = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hq = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.classifier = nn.Linear(num_hiddens,num_class)\n",
    "\n",
    "    def forward(self, x:torch.Tensor,h:torch.Tensor,c:torch.Tensor):  # X:[batch,seq_length,embedding]\n",
    "        out =[]\n",
    "        x = self.embedded(x)\n",
    "        length = x.shape[1]\n",
    "        x = x.permute(1,0,2)\n",
    "        for i in range(length):\n",
    "            Ft = self.act1(self.xf(x[i])+self.hf(h))\n",
    "            It = self.act1(self.xi(x[i])+self.hi(h))\n",
    "            Ot = self.act1(self.xo(x[i])+self.ho(h))\n",
    "            Ct1 = self.act2(self.xc(x[i])+self.hc(h))\n",
    "            Ct = Ft * c + Ct1 * It\n",
    "            ht = Ot * self.act2(Ct)\n",
    "            Y = self.hq(ht)\n",
    "            out.append(Y)\n",
    "            h = ht\n",
    "            c = Ct\n",
    "        return self.classifier(out[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_try(\n",
       "  (embedded): Embedding(95811, 128)\n",
       "  (xf): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (xi): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (xo): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (xc): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (hi): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (hf): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (ho): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (hc): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (hq): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (act2): Tanh()\n",
       "  (act1): Sigmoid()\n",
       "  (classifier): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lstm_try()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lstm_try()\n",
    "# x = torch.randn((32,20,128))\n",
    "# h = torch.randn((32,256))\n",
    "# c = torch.randn((32,256))\n",
    "# model(x,h,c)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class gru_try(nn.Module):\n",
    "    def __init__(self, emb_dim = 128,num_hiddens = 256) :\n",
    "        super(gru_try,self).__init__()\n",
    "        self.xz = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xr = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xh = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.hz = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hr = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hh = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hq = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.act1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x:torch.Tensor,h:torch.Tensor):  # X:[batch,seq_length,embedding]\n",
    "        out =[]\n",
    "        length = x.shape[1]\n",
    "        x = x.permute(1,0,2)\n",
    "        for i in range(length):\n",
    "            Zt = self.act1(self.xz(x[i])+self.hz(h))\n",
    "            Rt = self.act1(self.xr(x[i])+self.hr(h))\n",
    "            tmp = Rt * h\n",
    "            Ht1 = self.act2(self.xh(x[i])+self.hh(tmp))\n",
    "            ht = Zt * h + (1 - Zt) * Ht1\n",
    "            Y = self.hq(ht)\n",
    "            out.append(Y)\n",
    "            h = ht\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class gru_try(nn.Module):\n",
    "    def __init__(self, emb_dim = 128,num_hiddens = 256) :\n",
    "        super(gru_try,self).__init__()\n",
    "        self.embedded = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.xz = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xr = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.xh = nn.Linear(emb_dim,num_hiddens)\n",
    "        self.hz = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hr = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hh = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.hq = nn.Linear(num_hiddens,num_hiddens)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.calssifier = nn.Linear(num_hiddens,num_class)\n",
    "\n",
    "    def forward(self, x:torch.Tensor,h:torch.Tensor):  # X:[batch,seq_length,embedding]\n",
    "        out =[]\n",
    "        x = self.embedded(x)\n",
    "        length = x.shape[1]\n",
    "        x = x.permute(1,0,2)\n",
    "        for i in range(length):\n",
    "            Zt = self.act1(self.xz(x[i])+self.hz(h))\n",
    "            Rt = self.act1(self.xr(x[i])+self.hr(h))\n",
    "            tmp = Rt * h\n",
    "            Ht1 = self.act2(self.xh(x[i])+self.hh(tmp))\n",
    "            ht = Zt * h + (1 - Zt) * Ht1\n",
    "            Y = self.hq(ht)\n",
    "            out.append(Y)\n",
    "            h = ht\n",
    "        return self.calssifier(out[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gru_try()\n",
    "# x = torch.randn((32,20))\n",
    "# h = torch.randn((32,256))\n",
    "# c = torch.randn((32,256))\n",
    "# model(x,h)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=5, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_try()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        h = torch.randn((4,256))\n",
    "        c = torch.randn((4,256))\n",
    "        # print(type(h))\n",
    "        # print(text.shape)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text,h,c)\n",
    "        # predicted_label = model(text,h)\n",
    "        #print(predicted_label.shape, label.shape)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            h = torch.randn((4,256))\n",
    "            c = torch.randn((4,256))\n",
    "            predicted_label = model(text,h,c)\n",
    "            # predicted_label = model(text,h)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(5):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(test_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
